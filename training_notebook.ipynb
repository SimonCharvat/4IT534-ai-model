{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check python requirements\n",
    " - Python version between 3.8 and 3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries / packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Rescaling, RandomFlip, RandomRotation\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import custom file with different models\n",
    "Model structures are kept separately for easier testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset of images\n",
    "Dataset is downloaded from kaggle to cache. Path to the dataset is saved into variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"abdallahalidev/plantvillage-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "path = path + r\"\\plantvillage dataset\" + r\"\\color\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train, validation and test data\n",
    "Images are diveded into 3 splits. It is done physically on in the file structure. In working directory, folder is created where the imagas are copied and separeted into 3 subfolders.\n",
    "\n",
    "This step is ignored if the folder alredy exists - the split has already been done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "original_dataset_dir = path  # Path to the original dataset\n",
    "output_base_dir = 'split_dataset'  # Output directory for train, val, test\n",
    "\n",
    "# Skip if folder alredy exists\n",
    "if not os.path.isdir(output_base_dir):\n",
    "\n",
    "    # Create train, val, test directories\n",
    "    splits = ['train', 'validation', 'test']\n",
    "    for split in splits:\n",
    "        split_path = os.path.join(output_base_dir, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "\n",
    "    # Split ratios\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.1\n",
    "\n",
    "    # Split images\n",
    "    for class_name in os.listdir(original_dataset_dir):\n",
    "        class_path = os.path.join(original_dataset_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        # Create class directories in each split folder\n",
    "        for split in splits:\n",
    "            os.makedirs(os.path.join(output_base_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "        # Get all image files\n",
    "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Calculate split sizes\n",
    "        total_images = len(images)\n",
    "        train_size = int(total_images * train_ratio)\n",
    "        val_size = int(total_images * val_ratio)\n",
    "\n",
    "        # Assign images to splits\n",
    "        train_images = images[:train_size]\n",
    "        val_images = images[train_size:train_size + val_size]\n",
    "        test_images = images[train_size + val_size:]\n",
    "\n",
    "        # Function to copy images\n",
    "        def copy_images(image_list, split):\n",
    "            for image in image_list:\n",
    "                src = os.path.join(class_path, image)\n",
    "                dest = os.path.join(output_base_dir, split, class_name, image)\n",
    "                shutil.copy(src, dest)\n",
    "\n",
    "        # Copy images to respective folders\n",
    "        copy_images(train_images, 'train')\n",
    "        copy_images(val_images, 'validation')\n",
    "        copy_images(test_images, 'test')\n",
    "\n",
    "    print(\"Dataset successfully split!\")\n",
    "\n",
    "else:\n",
    "    print(f\"Folder {output_base_dir} already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images into code\n",
    "Images are loaded (prefetched) into memory as keras objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_dir = 'split_dataset/train'\n",
    "validation_dir = 'split_dataset/validation'\n",
    "test_dir = 'split_dataset/test'\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(224, 224),  # Resize all images to this size\n",
    "    batch_size=32          # Number of images per batch\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Optional: Prefetch for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation and normalization\n",
    "- Creates random variations for train set to prevent overfitting and make robust model\n",
    "- Rescales image values from 0-255 to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.1),\n",
    "])\n",
    "\n",
    "# Normalize the dataset\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x), y))\n",
    "train_dataset = train_dataset.map(lambda x, y: (Rescaling(1./255)(x), y))\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (Rescaling(1./255)(x), y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (Rescaling(1./255)(x), y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numbert of output classes (number of folders in data folder)\n",
    "num_of_classes = len(os.listdir(\"split_dataset/train\"))\n",
    "\n",
    "# Select model\n",
    "model_name, model = model_definition.get_model(\"ResNet50_v2\", num_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp to mark the model\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "checkpoint_filename = f\"checkpoints/model_{model_name}_{timestamp}_epoch_{{epoch:02d}}_acc_{{val_accuracy:.2f}}.keras\"\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filename,  # Save path\n",
    "    save_weights_only=False,  # Set to True to save only weights\n",
    "    save_best_only=False,     # Set to True to save only the best model\n",
    "    verbose=1                 # Print a message when saving\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=1,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "model.save(f'model/model_{model_name}_{timestamp}_epoch1_frozen_pretrained.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f'model/model_{model_name}_{timestamp}_epoch1_frozen_pretrained.keras')\n",
    "\n",
    "# Unfreeze the base pretrained model\n",
    "model.layers[0].trainable = True\n",
    "\n",
    "# Re-compile the Model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "model.save(f'model/model_{model_name}_{timestamp}_from_pretrained_finished.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot model training history of improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.keras\")\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to make labels readable\n",
    "Extract information from dataset class labels and transform them for better readlibility. Prints the result as a list of tupples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts information from the label\n",
    "def parse_label(label):\n",
    "    # Split the label into the flower name and disease part\n",
    "    label = label.split(\" \")[0]\n",
    "    parts = label.split(\"___\")\n",
    "    \n",
    "    # Extract the flower name and make it human-readable\n",
    "    flower_name = parts[0].replace(\"_\", \" \").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    # Determine if the label indicates a healthy plant\n",
    "    is_healthy = \"healthy\" in label\n",
    "    \n",
    "    # Extract the disease name or mark it as healthy\n",
    "    if is_healthy:\n",
    "        disease = \"healthy\"\n",
    "    else:\n",
    "        disease = parts[1].replace(\"_\", \" \").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    return flower_name, is_healthy, disease\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# print(parse_label(\"Corn_(maize)___Northern_Leaf_Blight\"))\n",
    "# print(parse_label(\"Cherry_(including_sour)___Powdery_mildew\"))\n",
    "# print(parse_label(\"Blueberry___healthy\"))\n",
    "# print(parse_label(\"Tomato___Spider_mites Two-spotted_spider_mite\"))\n",
    "\n",
    "import os\n",
    "\n",
    "base_path = './split_dataset/test'\n",
    "folder_names = os.listdir(base_path)  # List all items in the directory\n",
    "folder_names = [folder for folder in folder_names if os.path.isdir(os.path.join(base_path, folder))]  # Keep only directories\n",
    "\n",
    "result = []  # This will hold the nested list of results\n",
    "\n",
    "for folder in folder_names:\n",
    "    result.append(parse_label(folder))  # Apply the function and append to the result\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split model into parts\n",
    "GitHub and GitLab free accounts have 100 MB limit per file. To use the cloud, it is possible to split the file into multiple smaller files. And only merge them when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"model.keras\"\n",
    "\n",
    "def split_file(file_path, parts):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    part_size = len(content) // parts\n",
    "    for i in range(parts):\n",
    "        start = i * part_size\n",
    "        end = None if i == parts - 1 else (i + 1) * part_size\n",
    "        with open(f\"{file_path}.part{i+1}\", 'wb') as part_file:\n",
    "            part_file.write(content[start:end])\n",
    "\n",
    "split_file(file_name, parts=3)\n",
    "\n",
    "def merge_file(output_path, part_paths):\n",
    "    with open(output_path, 'wb') as output:\n",
    "        for part in part_paths:\n",
    "            with open(part, 'rb') as part_file:\n",
    "                output.write(part_file.read())\n",
    "\n",
    "# merge_file(\"model_final_new.keras\", [\"model_final.keras.part1\", \"model_final.keras.part2\", \"model_final.keras.part3\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
